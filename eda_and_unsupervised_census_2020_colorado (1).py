# -*- coding: utf-8 -*-
"""EDA_and_Unsupervised_Census_2020_Colorado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rtdCJG4GQ9eRJipRS2S9g2SbbNrFa-Jn

---
# **Data Description and Background:**

---

### High-Level Summary of the 2020 US Census Dataset

Note: the below data description is a summary of the kaggle data source. For a more detailed description please follow this link:

Source:

https://www.kaggle.com/datasets/zusmani/us-census-2020

#### Overview:
The 2020 US Census dataset is an extensive and comprehensive dataset encompassing the 24th decennial United States Census. Released publicly on August 12, 2021, the dataset provides a detailed snapshot of the demographic, geographic, and social attributes of the US population across all 50 states. The dataset includes approximately 12 GB of data and covers a wide array of variables that are essential for analyzing population trends, demographic shifts, and geographic distributions.

#### Content:
The dataset is composed of 50 files corresponding to each state. Each file contains numerous columns representing various geographic, demographic, and statistical attributes. The dataset includes over 400 columns, capturing detailed information such as:

- **Geographic Identifiers**: GEOID, GEOCODE, STATE (FIPS), COUNTY (FIPS), Census Tracts, and more.
- **Demographic Information**: Population counts by race, ethnicity, and combinations of racial groups.
- **Housing Data**: Housing unit counts, vacancy status, and types of housing structures.
- **Legislative and Political Boundaries**: Congressional districts, state legislative districts, and voting districts.
- **Urban and Rural Designations**: Urban areas, rural designations, and urban growth areas.
- **Institutional Data**: Counts of institutionalized populations, such as those in correctional facilities or nursing homes.
- **Regional Classifications**: Metropolitan and micropolitan areas, New England city and town areas, and regions.

#### Acknowledgements:
The dataset has been sourced from the US Census website and prepared for use on Kaggle by cleaning and organizing the data for accessibility, with no changes made to the original data.

Source:

https://www.kaggle.com/datasets/zusmani/us-census-2020

---

# **Write-up:**

---

## 1. Unsupervised Learning Problem Identification

**Problem Statement:**
The 2020 US Census dataset provides an opportunity to explore demographic patterns across the United States. This project focuses on the state of Colorado, aiming to uncover hidden patterns in the population through clustering analysis. By applying unsupervised learning techniques, we can identify distinct demographic segments within Colorado, which could inform policymakers, businesses, and community organizations about regional differences in population characteristics. This analysis is particularly relevant for understanding how different racial and ethnic groups are distributed across the state and how these distributions might influence various marketing strategies.

## 2. Data Collection and Cleaning Methods

The data for this project was sourced from the 2020 US Census, which offers a comprehensive snapshot of the US population. The dataset is publicly available on Kaggle and includes detailed demographic information for all states. For this analysis, the Colorado-specific file was extracted, providing demographic, geographic, and housing data at the census tract level. Before proceeding with the analysis, the data underwent preprocessing, including cleaning, handling missing values, and ensuring consistency in geographic identifiers, which are critical for accurate merging with geospatial data.

## 3. Model Building and Training Discussion

**Model Selection and Hyperparameter Tuning:**
In this analysis, K-Means clustering was chosen as the primary unsupervised learning method due to its simplicity and effectiveness in identifying distinct groups within the dataset. K-Means was applied to the standardized demographic data, and the number of clusters was determined based on the data's distribution. Additionally, Principal Component Analysis (PCA) and t-SNE were used for dimensionality reduction and visualization. PCA helped in understanding the variance explained by different components, while t-SNE provided a non-linear mapping that highlighted the separation between clusters. Hyperparameters, such as the number of clusters for K-Means and the perplexity for t-SNE, were carefully tuned to optimize the model's performance.

## 4. Results and Discussion

**Results Interpretation:**
The clustering analysis revealed several distinct demographic groups within Colorado. The clusters varied significantly in terms of racial and ethnic composition, with some clusters showing strong concentrations of specific demographic groups. For example, one cluster predominantly represented areas with a high Latino population, while others were more diverse or homogeneously composed of White or African American populations. The PCA and t-SNE visualizations provided further insights into the structure of the data, showing that certain clusters were well-separated, indicating clear demographic distinctions, while others overlapped, suggesting similarities in population characteristics. These findings can be valuable for targeted policy interventions and resource allocation.

## 5. Conclusion

**Conclusion:**
This analysis of Colorado's 2020 Census data using unsupervised learning techniques, specifically K-Means clustering, PCA, and t-SNE, has successfully identified distinct demographic segments within the state. The clusters revealed clear patterns in the distribution of racial and ethnic groups, which are critical for understanding the socio-economic dynamics of different regions. The well-separated clusters indicate areas with unique demographic characteristics, while the overlapping clusters suggest regions with shared population traits. These insights are valuable for a wide range of applications, from policymaking and community planning to business strategy. Future work could involve exploring additional clustering methods, such as DBSCAN or Gaussian Mixture Models, and considering more detailed socio-economic variables to further refine the analysis. Overall, this project demonstrates the power of unsupervised learning in extracting meaningful patterns from complex demographic data.
"""



"""---
# **Notebook:**

---

# **The below EDA and Unsupervised analysis is focused on one state: Colorado.**
## This state can be downloaded by going to the data explorer within the below link on Kaggle and downloading the file Colorado_CO.csv
https://www.kaggle.com/datasets/zusmani/us-census-2020?select=Colorado_CO.csv

# **Data Loading and Initial Setup**

# Before Unsupervised Learning can begin it is necessary to import python libraries and set up the environment. This notebook is built to run on Google Colab.

# Google Drive is mounted below. This is to gain access the dataset and related files. To change this csv file to load locally you first need to comment out the google colab mount drive line. Then where it loads the csv from a directory path just change the path to your local path.
"""

# Standard libraries
import numpy as np
import pandas as pd

# Geospatial libraries
import geopandas as gpd
import folium
from folium.plugins import HeatMap

# Plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Machine learning libraries
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Widgets and display libraries
import ipywidgets as widgets
from IPython.display import display, clear_output

# Google Colab specific imports
from google.colab import drive


# Mount Google Drive
drive.mount('/content/drive')

"""# **Exploratory Data Analysis (EDA)**

# This section involves loading the dataset into a Pandas DataFrame and performing initial exploratory data analysis.
# EDA is essential for understanding the basic structure, distribution, and characteristics of the data.
# It helps in identifying potential issues like missing values, outliers, and provides a statistical summary of the data.

"""

# Load the data
data = pd.read_csv('/content/drive/MyDrive/Unsupervised_Machine_Learning/Colorado_CO.csv')

display(data)

data.shape

# Display statistical summary of the DataFrame
data.describe()



"""# **Column Information Extraction**

# Here, the column names and descriptions are extracted from a separate text file.
# This step is useful for better understanding the data attributes, especially when dealing with datasets that have non-intuitive column names.
# Proper documentation of columns ensures clarity when performing analysis and makes it easier to communicate findings.

## The column_descriptions.txt file below was sourced from this Kaggle page:  https://www.kaggle.com/datasets/zusmani/us-census-2020  (about the data --> content)
"""

# Step 1: Read the text file and extract column names

# The idea here is to compare the Colorado_CO.csv column headers to the data description column names to ensure they are the same.
#  As with some documentation the below does not match 100%.
# Within the remainder of this notebook these columns are not used so this can be ignored, at least for this analysis:
#   The following column names do not match any key in the text file:

# I needed a way to easily check if the data descriptions on kaggle matched the column names within the CSV on kaggle:

#mismatch
#CIFSN_x
#mismatch
#CIFSN_y
#mismatch
#CIFSN_x.1
#mismatch
#CIFSN_y.1


column_info = {}

with open('/content/drive/MyDrive/Unsupervised_Machine_Learning/column_descriptions.txt', 'r') as file:
    for line in file:
        parts = line.strip().split(' ', 1)  # Split into column name and description
        column_name = parts[0]  # The column name is the first part

        column_description = parts[1] if len(parts) > 1 else ''
        column_name_and_description = f"{column_name} | {column_description}"
        column_info[column_name] = column_name_and_description # The description is the second part (if available)
        if column_name.strip() == "TRACT":
          print(line)
          print(column_name)
          print(column_description)
          print(column_name_and_description)
          print("\n\n")


# Step 2: Compare original DataFrame column names with the keys in column_info
mismatches = []

for col in data.columns:
    # Check if the original column name is found in the dictionary keys
    if col not in column_info:
        mismatches.append(col)

# Step 3: Output the results
if mismatches:
    print("The following column names do not match any key in the text file:")
    for mismatch in mismatches:
      print("mismatch")
      print(mismatch)
else:
    print("All column names in the DataFrame match the column names from the text file.")

print(data.columns)

"""# **Latino Population Column Identification**

# This part of the notebook identifies columns related to the Latino population based on the descriptions extracted earlier.
# This is particularly useful when focusing on demographic analysis related to specific ethnic or racial groups.

"""

# Assuming column_info is already populated
latino_columns = [col for col, desc in column_info.items() if 'latino' in desc.lower()]

# Output the results
if latino_columns:
    print("Columns with 'latino' in their descriptions:")
    for col in latino_columns:
        print(f"Column: {col} - Description: {column_info[col]}")
else:
    print("No columns with 'latino' in their descriptions were found.")





"""# **Geospatial Data Handling**

# Geospatial data is critical when analyzing demographic information by region.
# This section loads a shapefile representing Colorado's geographic regions and prepares it for merging with the demographic data.
# Handling geospatial data allows for more accurate visual representation of demographic trends on maps.

"""

# Load Colorado shapefile for counties (or other relevant regions)

colorado_shapefile = gpd.read_file('/content/drive/MyDrive/Unsupervised_Machine_Learning/geo_files/cb_2018_08_bg_500k.shp')
display(colorado_shapefile)



colorado_shapefile.describe()



# Show column names and their data types
print(colorado_shapefile.dtypes)







"""# **Data Cleaning and Merging**

# Before performing any analysis, it's important to ensure that the data is clean and properly formatted.
# Ensure that geographic identifiers are numeric and consistent across datasets, and then merge the demographic data with the geospatial data.
# This step is crucial for creating accurate maps and performing any spatial analysis.

"""

# Ensure both GEOID and GEOCODE are numeric
colorado_shapefile['GEOID'] = colorado_shapefile['GEOID'].astype(float)

# Use pd.to_numeric with errors='coerce' to convert GEOCODE to numeric, forcing invalid parsing to NaN
data['GEOCODE'] = pd.to_numeric(data['GEOCODE'], errors='coerce')

# Drop rows where GEOCODE is NaN (which means they couldn't be cast to numeric)
data = data.dropna(subset=['GEOCODE'])

data['GEOCODE'] = data['GEOCODE'].astype(float)

data.to_csv('/content/drive/MyDrive/Unsupervised_Machine_Learning/data_df_export.csv', index=False)

# Export the colorado_shapefile GeoDataFrame to a CSV file
colorado_shapefile.to_csv('/content/drive/MyDrive/Unsupervised_Machine_Learning/colorado_shapefile.csv', index=False)

# Merge the data with the shapefile based on a common geographic identifier
merged = colorado_shapefile.merge(data, how='inner', left_on='GEOID', right_on='GEOCODE')

latino_population = merged[['geometry', 'P0020002']]

display(latino_population)

# Count NaN values in each column of the latino_population DataFrame
nan_counts = latino_population.isna().sum()

# Display the result
print(nan_counts)

latino_population.shape



"""
# **Heatmap Visualization**

# Heatmaps are a powerful tool for visualizing the distribution of demographic data across geographical regions.
# In this section, a heatmap is used to visualize the distribution of the Latino population across Colorado.
# This visualization helps identify areas with higher concentrations of the Latino population and can be useful for targeted outreach or policy-making.
"""

# Create the map centered around Colorado
map_center = [39.5501, -105.7821]  # Approximate center of Colorado
m = folium.Map(location=map_center, zoom_start=7)

# Convert to the format needed for HeatMap
heat_data = [[point.xy[1][0], point.xy[0][0], pop] for point, pop in zip(latino_population.geometry.centroid, latino_population['P0020002']) if pop > 0]

# Add HeatMap to the map
HeatMap(heat_data, radius=15, max_zoom=13).add_to(m)

# Save the map to an HTML file or display it
m.save('/content/drive/MyDrive/Unsupervised_Machine_Learning/colorado_latino_population_heatmap.html')
m



"""# **Interactive Data Exploration**

# This section introduces interactive visualizations for exploring the dataset.
# Users can select columns to visualize their distributions or generate heatmaps dynamically.
# Interactive data exploration is beneficial as it allows users to gain insights in real-time and adjust their analysis based on observations.

"""







# Create a dropdown widget with column names and descriptions, defaulting to "Latino" option
dropdown = widgets.Dropdown(
    options=[(desc, col) for col, desc in column_info.items()],
    description='Select Column:',
    style={'description_width': 'initial'},
    value='P0020002'  # Default value for "Latino" selection
)

# Function to update the heatmap based on dropdown selection
def update_map(column):
    # Filter to include only relevant data
    selected_population = merged[['geometry', column]]

    # Re-project to WGS84 (EPSG:4326) for displaying on the web map
    selected_population = selected_population.to_crs(epsg=4326)

    # Create the map centered around Colorado
    map_center = [39.5501, -105.7821]  # Approximate center of Colorado
    m = folium.Map(location=map_center, zoom_start=7)

    # Convert to the format needed for HeatMap
    heat_data = [[point.xy[1][0], point.xy[0][0], pop] for point, pop in zip(selected_population.geometry.centroid, selected_population[column]) if pop > 0]

    # Add HeatMap to the map
    HeatMap(heat_data, radius=15, max_zoom=13).add_to(m)

    # Display the map
    clear_output(wait=True)
    display(m)

# Display the dropdown and bind it to the update function
widgets.interactive(update_map, column=dropdown)



display(data.columns)

# Create a text search box and a dropdown widget
search_box = widgets.Text(
    description='Search:',
    style={'description_width': 'initial'},
    placeholder='Type to search...'
)

dropdown = widgets.Dropdown(
    options=[(desc, col) for col, desc in column_info.items()],
    description='Select Column:',
    style={'description_width': 'initial'},
    value='P0020002'  # Default value for "Latino" selection
)

# Function to filter the dropdown based on search query
def filter_options(change):
    search_query = change['new'].lower()
    filtered_options = [(desc, col) for col, desc in column_info.items() if search_query in desc.lower()]
    dropdown.options = filtered_options

# Attach the filter function to the search box
search_box.observe(filter_options, names='value')

# Function to update the heatmap based on dropdown selection
def update_map(column):
    # Filter to include only relevant data
    selected_population = merged[['geometry', column]]

    # Re-project to WGS84 (EPSG:4326) for displaying on the web map
    selected_population = selected_population.to_crs(epsg=4326)

    # Create the map centered around Colorado
    map_center = [39.5501, -105.7821]  # Approximate center of Colorado
    m = folium.Map(location=map_center, zoom_start=7)

    # Convert to the format needed for HeatMap
    heat_data = [[point.xy[1][0], point.xy[0][0], pop] for point, pop in zip(selected_population.geometry.centroid, selected_population[column]) if pop > 0]

    # Add HeatMap to the map
    HeatMap(heat_data, radius=15, max_zoom=13).add_to(m)

    # Display the map
    clear_output(wait=True)
    display(m)

# Display the search box, dropdown, and bind it to the update function
display(search_box, dropdown)
widgets.interactive(update_map, column=dropdown)



"""# **Clustering:**

# What it is doing:
# This section applies K-Means clustering to the standardized demographic data to identify distinct clusters of the population based on race and ethnicity. It then visualizes the distribution of these clusters.

# Why it could be useful:
# Clustering can reveal hidden patterns in demographic data by grouping similar regions together. This is particularly useful for businesses and policymakers who need to understand regional differences in population characteristics to make informed decisions about resource allocation, marketing strategies, and service provision.

"""





# Select columns related to race and ethnicity
columns_of_interest = [
    'P0010003',  # White alone
    'P0010004',  # Black or African American alone
    'P0010005',  # American Indian and Alaska Native alone
    'P0010006',  # Asian alone
    'P0010007',  # Native Hawaiian and Other Pacific Islander alone
    'P0010008',  # Some Other Race alone
    'P0010009',  # Population of two or more races
]

# Filter data to include only columns of interest
race_data = merged[columns_of_interest]

# Handle missing values (e.g., fill with 0 or use other imputation methods)
race_data.fillna(0, inplace=True)

# Standardize the data
scaler = StandardScaler()
race_data_scaled = scaler.fit_transform(race_data)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=7, random_state=42)  # Adjust the number of clusters as needed
kmeans.fit(race_data_scaled)

# Add the cluster labels back to the original data
merged['Cluster'] = kmeans.labels_

# Visualize the clusters
plt.figure(figsize=(10, 6))
sns.countplot(x='Cluster', data=merged, palette='viridis')
plt.title('Distribution of Clusters')
plt.show()

# Optionally, visualize clusters on a map (requires additional steps to join with geographic data)



"""# **Analyzing and Visualizing Clusters**

## What it is doing:
This section analyzes one of the clusters by calculating its centroid and visualizing the distribution of demographic features within that cluster.

## Why it could be useful:
Analyzing clusters helps to understand the characteristics of specific segments of the population. By examining the centroid and distributions within a cluster, you can gain insights into the common attributes that define the group, which can be used for targeted interventions or services.

## In the case below, I review cluster 1 as it is the dominant cluster within the data. From a business perspective, there could be financial reasons to target cluster 1. The below shows the distributions within that cluster. For example, there are few Native Hawaiian/Pacific Islander individuals within cluster 1. From a marketing perspective, it would save costs if the company removed any targeting campaigns for Native Hawaiians within cluster 1, for example.


"""

# Get data points in Cluster 1
cluster_1_data = merged[merged['Cluster'] == 1]

# Calculate the centroid of Cluster 1
centroid_1 = cluster_1_data[columns_of_interest].mean()

print("Centroid of Cluster 1:")
print(centroid_1)

# Descriptive statistics of Cluster 1
print("\nDescriptive Statistics of Cluster 1:")
print(cluster_1_data[columns_of_interest].describe())

# Visualizing the features within Cluster 1
import matplotlib.pyplot as plt
import seaborn as sns

# Create histograms for each feature in Cluster 1
for col in columns_of_interest:
    plt.figure(figsize=(8, 4))
    sns.histplot(cluster_1_data[col], bins=30, kde=True)
    plt.title(f'Distribution of {col} in Cluster 1')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()



"""# Distribution of demographic features within cluster 1 (with descriptions)"""

# Create histograms for each feature in Cluster 1 with title including both column name and description
for col in columns_of_interest:
    plt.figure(figsize=(8, 4))

    # Get the column description from the dictionary
    title = column_info.get(col, col)  # Default to column name if description is missing

    sns.histplot(cluster_1_data[col], bins=30, kde=True)
    plt.title(f'Distribution of {title} in Cluster 1')
    plt.xlabel(title)
    plt.ylabel('Frequency')
    plt.show()







# Select relevant columns for PCA (could be the same as used in clustering or different based on your analysis)
columns_of_interest = [
    'P0010003',  # White alone
    'P0010004',  # Black or African American alone
    'P0010005',  # American Indian and Alaska Native alone
    'P0010006',  # Asian alone
    'P0010007',  # Native Hawaiian and Other Pacific Islander alone
    'P0010008',  # Some Other Race alone
    'P0010009',  # Population of two or more races
]

# Prepare the data for PCA
pca_data = merged[columns_of_interest].fillna(0)
pca_data_scaled = StandardScaler().fit_transform(pca_data)

# Apply PCA
pca = PCA(n_components=2)  # Reduce to 2 components for easy visualization
pca_result = pca.fit_transform(pca_data_scaled)

# Add PCA results back to the original DataFrame
merged['PCA1'] = pca_result[:, 0]
merged['PCA2'] = pca_result[:, 1]

# Plot the PCA results
plt.figure(figsize=(10, 6))
plt.scatter(merged['PCA1'], merged['PCA2'], c=merged['Cluster'], cmap='viridis', s=50, alpha=0.7)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Demographic Data with Cluster Labels')
plt.colorbar(label='Cluster')
plt.show()

# Explained variance by each principal component
explained_variance = pca.explained_variance_ratio_
print(f"Explained variance by first component: {explained_variance[0]:.2f}")
print(f"Explained variance by second component: {explained_variance[1]:.2f}")

"""### Insights from the PCA Plot

1. **Cluster Separation**:
   - Clusters 0, 3, 4, and 6 show some degree of separation in the PCA space, with clusters 0 and 3 being tightly grouped, indicating similar demographics.

2. **Overlapping Clusters**:
   - Clusters 1, 2, and 5 overlap more, suggesting these clusters may require more components to be distinctly separated.

3. **Outliers**:
   - A few points in clusters 2 and 3 are distant from the main group, possibly indicating outliers with unique demographic characteristics.

4. **Cluster 6**:
   - Cluster 6 (yellow) is well-separated, especially along the first principal component, highlighting distinct demographic traits.

### Next Steps:
- **Explore Additional Components**: Consider analyzing the third and fourth principal components to capture more variance.
- **Investigate Demographic Features**: Examine which demographic features drive the separation of clusters, especially in cluster 6.
- **Consider Alternative Clustering Methods**: Methods like DBSCAN or Gaussian Mixture Models might provide different insights, especially given the overlapping clusters.

"""





# Prepare the data for t-SNE
columns_of_interest = [
    'P0010003',  # White alone
    'P0010004',  # Black or African American alone
    'P0010005',  # American Indian and Alaska Native alone
    'P0010006',  # Asian alone
    'P0010007',  # Native Hawaiian and Other Pacific Islander alone
    'P0010008',  # Some Other Race alone
    'P0010009',  # Population of two or more races
]

tsne_data = merged[columns_of_interest].fillna(0)
tsne_data_scaled = StandardScaler().fit_transform(tsne_data)

# Apply t-SNE
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
tsne_result = tsne.fit_transform(tsne_data_scaled)

# Add t-SNE results back to the original DataFrame
merged['TSNE1'] = tsne_result[:, 0]
merged['TSNE2'] = tsne_result[:, 1]

# Plot the t-SNE results
plt.figure(figsize=(10, 6))
plt.scatter(merged['TSNE1'], merged['TSNE2'], c=merged['Cluster'], cmap='viridis', s=50, alpha=0.7)
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.title('t-SNE of Demographic Data with Cluster Labels')
plt.colorbar(label='Cluster')
plt.show()

"""### t-SNE Plot Analysis:

#### Cluster Distribution:
- The clusters are distributed across the t-SNE space, with some clusters being more tightly grouped than others.
- Clusters with tighter groupings suggest more homogeneous data within those clusters, while more spread-out clusters may indicate greater diversity or overlap in demographic features.

#### Separation of Clusters:
- There is noticeable separation between some clusters, indicating that t-SNE has successfully identified regions of the data that are distinct in terms of their demographic composition.
- For example, clusters represented by yellow and green seem to be well-separated, suggesting distinct demographic characteristics in those areas.

#### Overlapping Clusters:
- Some clusters overlap or are positioned very close to each other, indicating that these groups share some common demographic features. This might suggest that further refinement, such as increasing the number of clusters or using additional components in PCA, might help in distinguishing these groups more clearly.

#### Outliers and Spread:
- The presence of scattered points around the main cluster bodies could indicate outliers or regions with unique demographic characteristics. These might be worth investigating separately to understand what makes them different.

"""













# Create a dropdown widget for selecting clusters
cluster_dropdown = widgets.Dropdown(
    options=[(f'Cluster {i}', i) for i in range(7)],  # Assuming there are 7 clusters
    description='Select Cluster:',
    style={'description_width': 'initial'}
)

# Function to update the histograms based on the selected cluster
def update_histograms(cluster):
    clear_output(wait=True)  # Clear the previous output

    # Filter data for the selected cluster
    cluster_data = merged[merged['Cluster'] == cluster]

    # Number of columns (features) to plot
    num_cols = len(columns_of_interest)

    # Determine the number of rows and columns for subplots
    num_rows = (num_cols + 2) // 3  # This will create up to 3 plots per row

    # Create a figure with subplots
    fig, axes = plt.subplots(num_rows, 3, figsize=(15, 5 * num_rows))  # Adjust the figure size as needed
    axes = axes.flatten()  # Flatten the 2D array of axes into a 1D array for easier iteration

    # Loop through each feature and plot it in the corresponding subplot
    for i, col in enumerate(columns_of_interest):
        title = column_info.get(col, col)  # Get the column description
        sns.histplot(cluster_data[col], bins=30, kde=True, ax=axes[i])
        axes[i].set_title(f'Distribution of {title} in Cluster {cluster}')
        axes[i].set_xlabel(title)
        axes[i].set_ylabel('Frequency')

    # Remove any empty subplots (if the number of columns is not a multiple of 3)
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    # Adjust layout to prevent overlap
    plt.tight_layout()
    plt.show()

# Display the dropdown and bind it to the update function
widgets.interactive(update_histograms, cluster=cluster_dropdown)
display(cluster_dropdown)

update_histograms(cluster=cluster_dropdown.value)

# Create a dropdown widget for selecting a column
column_dropdown_of_interest = widgets.Dropdown(
    options=[(column_info[col], col) for col in columns_of_interest],
    description='Select Column:',
    style={'description_width': 'initial'}
)

# Function to update the histogram based on the selected column
def update_histogram_column_of_interest(column):
    clear_output(wait=True)  # Clear the previous output

    # Create subplots for 7 clusters (3x3 grid, only using 7 plots)
    fig, axes = plt.subplots(3, 3, figsize=(18, 12), sharey=True)
    axes = axes.flatten()  # Flatten the 2D array of axes to iterate easily

    for i in range(7):  # Only loop through 7 clusters
        ax = axes[i]

        # Filter data for the current cluster
        cluster_data = merged[merged['Cluster'] == i]

        # Get the column description from the dictionary
        title = column_info.get(column, column)  # Default to column name if description is missing

        # Plot the histogram in the corresponding subplot
        sns.histplot(cluster_data[column], bins=30, kde=True, ax=ax)
        ax.set_title(f'Cluster {i}')
        ax.set_xlabel(title)
        ax.set_ylabel('Frequency')

    # Hide the last two unused subplots
    for i in range(7, 9):
        axes[i].set_visible(False)

    # Adjust layout to prevent overlap
    plt.suptitle(f'Distribution of {title} across Clusters')
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

# Display the dropdown and bind it to the update function
widgets.interactive(update_histogram_column_of_interest, column=column_dropdown_of_interest)
display(column_dropdown_of_interest)

update_histogram_column_of_interest(column_dropdown_of_interest.value)



# Create a dropdown widget for selecting clusters
cluster_dropdown_geo_location = widgets.Dropdown(
    options=[(f'Cluster {i}', i) for i in range(7)],  # Assuming there are 7 clusters
    description='Select Cluster:',
    style={'description_width': 'initial'}
)

# Function to update the map based on the selected cluster
def update_map(cluster):
    clear_output(wait=True)  # Clear the previous output

    # Filter data for the selected cluster
    cluster_data = merged[merged['Cluster'] == cluster]

    # Create a map centered around Colorado
    map_center = [39.5501, -105.7821]  # Approximate center of Colorado
    m = folium.Map(location=map_center, zoom_start=7)

    # Add polygons for each geometry in the selected cluster
    folium.GeoJson(cluster_data[['geometry']]).add_to(m)

    # Add HeatMap (optional, to show concentration)
    heat_data = [[point.xy[1][0], point.xy[0][0]] for point in cluster_data.geometry.centroid]
    HeatMap(heat_data, radius=10, max_zoom=13).add_to(m)

    # Display the map
    display(m)

# Display the dropdown and bind it to the update function
widgets.interactive(update_map, cluster=cluster_dropdown_geo_location)
display(cluster_dropdown_geo_location)

update_map(cluster=cluster_dropdown_geo_location.value)



# Sum the values for each column across all GEOIDs
column_sums = merged[columns_of_interest].sum()

# Calculate the total sum across all columns
total_sum = column_sums.sum()

# Calculate the percentage for each column based on the total sum
percentages = (column_sums / total_sum) * 100

# Print out the percentages with both name and description
print("Percentage Composition (should sum to 100%):")
for col, pct in percentages.items():
    description = column_info.get(col, col)  # Get the description, defaulting to the column name if not found
    print(f"{col} | {description}: {pct:.2f}%")

# Optionally, convert to a DataFrame for better visualization
percentages_df = pd.DataFrame({
    'Description': [column_info.get(col, col) for col in percentages.index],
    'Percentage': percentages.values
})
print("\nPercentages DataFrame:")
print(percentages_df)

import matplotlib.pyplot as plt

# Assuming percentages_df is your DataFrame containing the percentage data
# and `Description` is the column with the labels, and `Percentage` has the calculated values

# Data for plotting
labels = percentages_df['Description']
percentages = percentages_df['Percentage']

# Define a list of colors for each segment
colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF66B2', '#FFD700', '#ADFF2F']

# Create a figure and axis
fig, ax = plt.subplots(figsize=(10, 6))

# Plot a horizontal stacked bar with different colors
left = 0
for i in range(len(percentages)):
    ax.barh(y=0, width=percentages.iloc[i], left=left, color=colors[i], edgecolor='black', label=labels.iloc[i])
    left += percentages.iloc[i]

# Add the legend, label, and title
ax.legend(title='Demographics', bbox_to_anchor=(1.05, 1), loc='upper left')
ax.set_xlabel('Percentage')
ax.set_title('100% Stacked Bar Chart of Demographic Composition')

# Hide y-axis labels and ticks since there's only one bar
ax.get_yaxis().set_visible(False)

# Display the plot
plt.show()



